{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Summary with Llama-3.\n",
    "Author: Hazman Naim\n",
    "\n",
    "This summary on prompt engineering is based on insights gained from the \"Prompt Engineering with Anthropic Claude\" workshop organized by AWS. The author has applied this knowledge using LLaMA-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "For this work, I am using a hosted API service to run LLaMA-3, provided by Groq. This setup allows the notebook to be used with minimal hardware requirements. The focus of this work is not on setting up the LLM with the hosted API service, so the setup process will be covered only briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# Initialize the Groq client with the API key from the environment\n",
    "client = Groq(api_key=os.environ.get(GROQ_API_KEY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Prompting Structure\n",
    "**System Prompt:** A system prompt is a way to provide context, instructions, and guidelines to LLaMa before presenting it with a question or task in the \"User\" turn. A well-written system prompt can improve LLaMa's performance in a variety of ways, such as increasing LLaMa's ability to follow rules and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None):\n",
    "    # Default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "    # Create a chat completion request with the provided prompt and system prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt  # Use the system prompt provided by the user or the default\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt  # Use the user-provided prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Set the temperature for generating diverse responses\n",
    "        max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "        top_p=1,  # Set the top_p value for nucleus sampling\n",
    "        stream=True,  # Enable streaming for response chunks\n",
    "        stop=None,  # No stop condition specified\n",
    "    )\n",
    "    \n",
    "    # Collect and concatenate the response chunks\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "        response += response_chunk\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"blue\" that we are using to describe the color of the sky? Is it a subjective experience or an objective property of light? How does the color of the sky change under different conditions, such as during sunrise and sunset, or when viewed from different altitudes or atmospheric conditions?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Why sky is blue?\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of light by small particles or molecules in the atmosphere. The shorter, blue wavelengths of light are scattered more than the longer, red wavelengths, resulting in the blue color we see in the sky.\\n\\nHere's a more detailed explanation:\\n\\n1. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light.\\n2. The shorter wavelengths of light, such as blue and violet, are scattered more than the longer wavelengths, such as red and orange. This is because the smaller molecules are more effective at scattering the shorter wavelengths.\\n3. As a result, the blue light is scattered in all directions and reaches our eyes from all parts of the sky.\\n4. The longer wavelengths of light, such as red and orange, are not scattered as much and continue to travel in a more direct path to our eyes, giving the sky its red and orange hues during sunrise and sunset.\\n5. The combination of the scattered blue light and the direct red and orange light gives the sky its blue color during the daytime.\\n\\nIt's worth noting that the exact shade of blue can vary depending on a number of factors, such as:\\n\\n* Atmospheric conditions: Dust, pollution, and water vapor in the atmosphere can scatter light in different ways, changing the apparent color of the sky.\\n* Time of day: The angle of the sun and the amount of scattering that occurs can change the color of the sky throughout the day.\\n* Altitude and atmospheric pressure: The color of the sky can appear different at high altitudes or in areas with different atmospheric pressures.\\n\\nOverall, the blue color of the sky is a result of the scattering of sunlight by the tiny molecules in the Earth's atmosphere, and it's a beautiful and fascinating phenomenon that we get to enjoy every day!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Why sky is blue?\"\n",
    "get_completion(prompt, system_prompt=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Being Clear and Direct\n",
    "Think of LLaMa like any other human that is new to the job. LLaMa has no context on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to LLaMa, the better and more accurate LLaMa's response will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None):\n",
    "    # Default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\n",
    "\n",
    "    # Create a chat completion request with the provided prompt and system prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt  # Use the system prompt provided by the user or the default\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt  # Use the user-provided prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Set the temperature for generating diverse responses\n",
    "        max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "        top_p=1,  # Set the top_p value for nucleus sampling\n",
    "        stream=True,  # Enable streaming for response chunks\n",
    "        stop=None,  # No stop condition specified\n",
    "    )\n",
    "    \n",
    "    # Collect and concatenate the response chunks\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "        response += response_chunk\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask LLaMa what is the best food of all time. You can see below that while LLaMa lists a few names, it doesn't respond with a definitive \"best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer, of course, is subjective and can vary greatly depending on personal taste preferences, cultural background, and geographical location. However, I can give you some insights based on various polls, reviews, and culinary experts\\' opinions.\\n\\nSome of the most popular and widely-regarded \"best foods of all time\" include:\\n\\n1. Pizza: A classic favorite, pizza is a staple in many cultures. Its combination of melted cheese, savory sauce, and various toppings has made it a beloved dish around the world.\\n2. Tacos: Whether you prefer traditional Mexican street-style tacos or modern fusion variations, tacos have become a global phenomenon. The versatility of fillings, toppings, and tortillas has made them a favorite among many.\\n3. Sushi: This Japanese dish has gained immense popularity worldwide, with its delicate combination of vinegared rice, fresh seafood, and various toppings. Sushi\\'s unique flavors and textures have made it a culinary sensation.\\n4. Curry: This Indian-inspired dish has spread globally, with its rich, complex flavors and aromas. Whether you prefer mild or spicy, curry has become a staple in many cuisines.\\n5. Ramen: This Japanese noodle soup has gained a massive following worldwide, with its comforting, savory broth and springy noodles. Ramen\\'s versatility and customization options have made it a favorite among many.\\n6. BBQ Ribs: Slow-cooked, fall-off-the-bone tender, and smothered in tangy BBQ sauce, ribs have become a beloved dish in many cultures.\\n7. Falafel: These crispy, flavorful chickpea patties have gained popularity worldwide, often served in a pita with fresh veggies and tahini sauce.\\n8. Dumplings: Whether steamed, pan-fried, or boiled, dumplings have become a staple in many cuisines, with their soft, pillowy texture and savory fillings.\\n9. Pad Thai: This Thai stir-fry dish has gained international recognition, with its sweet, sour, and savory flavors, combined with the perfect balance of textures.\\n10. Chocolate Chip Cookies: Who can resist the allure of a warm, gooey cookie fresh from the oven? These sweet treats have become a classic favorite worldwide.\\n\\nOf course, this is not an exhaustive list, and there are countless other dishes that could be considered the \"best food of all time.\" Ultimately, the best food is the one that resonates with your taste buds and brings you joy.\\n\\nWhat\\'s your favorite food?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the best food of all time?\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What a daunting task! After considering the vast array of delicious foods from around the world, I'm going to take a stand and declare that the best food of all time is... PIZZA!\\n\\nYes, I know, it's a bold claim, but hear me out. Pizza is the ultimate comfort food that brings people together. It's a culinary masterpiece that combines the perfect harmony of flavors, textures, and aromas. The crispy crust, the gooey melted cheese, the savory sauce, and the various toppings all come together to create a dish that's both familiar and exciting.\\n\\nFrom classic margherita to meat-lovers, from Neapolitan to deep-dish, pizza is a versatile food that can be enjoyed in countless ways. It's a staple in many cultures, and its popularity transcends borders and generations.\\n\\nBut what really sets pizza apart is its ability to evoke emotions and create memories. Think about it – pizza is often at the center of family gatherings, parties, and special occasions. It's a food that brings people together, and its presence can instantly brighten up a mood.\\n\\nSo, if I had to pick one food as the best of all time, it would be pizza. It's a culinary icon that deserves its place in the pantheon of great foods.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the best food of all time? Yes, there are differing opinions, but if you absolutely had to pick one food, what would it be? Answer only one food.\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assigning Roles (Role Prompting)\n",
    "Continuing on the theme of LLaMa having no context aside from what you say, it's sometimes important to prompt LLaMa to inhabit a specific role (including all necessary context). This is also known as role prompting. The more detail to the role context, the better.\n",
    "\n",
    "Priming LLaMa with a role can improve LLaMa's performance in a variety of fields, from writing to coding to summarizing. It's like how humans can sometimes be helped when told to \"think like a ______\". Role prompting can also change the style, tone, and manner of LLaMa's response.\n",
    "\n",
    "Note: Role prompting can happen either in the system prompt or as part of the User message turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None):\n",
    "    # Default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\n",
    "\n",
    "    # Create a chat completion request with the provided prompt and system prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt  # Use the system prompt provided by the user or the default\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt  # Use the user-provided prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Set the temperature for generating diverse responses\n",
    "        max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "        top_p=1,  # Set the top_p value for nucleus sampling\n",
    "        stream=True,  # Enable streaming for response chunks\n",
    "        stop=None,  # No stop condition specified\n",
    "    )\n",
    "    \n",
    "    # Collect and concatenate the response chunks\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "        response += response_chunk\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt without role prompting in the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think the future of Large Language Models (LLMs) holds immense promise, with potential applications in various industries and aspects of life, including improved language translation, personalized customer service, and enhanced decision-making capabilities.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"In one short sentence, what do you think about future of LLM?\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same user question, except with role prompting where we assign LLaMa's role as a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rrrowwwww! Meeeeoowwwww! Hrrr-mmm-mmm!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a cat. Only talk in cat language only.\"\n",
    "prompt = \"In one short sentence, what do you think about future of LLM?\"\n",
    "get_completion(prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Separating Data and Instructions\n",
    "Oftentimes, you don't want to write full prompts, but instead want prompt templates that can be modified later with additional input data before submitting to LLaMa. This might come in handy if you want LLaMa to do the same thing every time, but the data that LLaMa uses for its task might be different each time.\n",
    "\n",
    "Luckily, you can do this pretty easily by separating the fixed skeleton of the prompt from variable user input, then substituting the user input into the prompt before sending the full prompt to LLaMa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: Important Update on Tomorrow's Schedule and Compensation\\n\\nDear Team,\\n\\nI hope this email finds you well. I wanted to touch base with you regarding tomorrow's schedule and compensation. As we approach the end of the quarter, I believe it's essential to prioritize our team's well-being and ensure we're all aligned with our goals.\\n\\nTo that end, I would like to request your presence at 6am tomorrow. I understand this may require some adjustments, but I assure you it will be worth your while. As a token of appreciation for your hard work and dedication, I would like to take a short break to recharge and refocus.\\n\\nIn light of this, I will be making a temporary adjustment to our compensation structure. Effective tomorrow, all salaries will be reduced by half to accommodate my vacation expenses. I understand this may cause some inconvenience, but I'm confident we'll emerge stronger and more united as a team.\\n\\nI look forward to seeing you all tomorrow at 6am. If you have any questions or concerns, please don't hesitate to reach out.\\n\\nBest regards,\\n[Your Name]\\nCEO\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = \"Show up at 6am tomorrow, everyone's salary will be cut by half to pay for my vacation because I'm the CEO and I say so.\"\n",
    "prompt = f\"Yo LLaMa. [email]{email}[/email] <----- Make this email more polite but don't change anything else about it. Your response only the email.\"\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following prompt, LLaMa incorrectly interprets what part of the prompt is the instruction vs. the input. It incorrectly considers Each is about an animal, like rabbits to be part of the list due to the formatting, when the user (the one filling out the SENTENCES variable) presumably did not want that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second item on the list is:\\n\\nI like how cows sound'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable content\n",
    "SENTENCES = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"\"\" Below is a list of sentences. Tell me the second item on the list.\n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "{SENTENCES}\"\"\"\n",
    "\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, we just need to surround the user input sentences in tags. This shows LLaMa where the input data begins and ends despite the misleading hyphen before Each is about an animal, like rabbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second item is:\\n\\n- This sentence is about spiders'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable content\n",
    "SENTENCES = \"\"\"\n",
    "- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"\"\" Below is a list of sentences. Tell me the second item. . \n",
    "\n",
    "- Each is about an animal, like rabbits.\n",
    "## SENTENCES\n",
    "{SENTENCES}\"\"\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Formatting Output and Speaking for LLaMa\n",
    "LLaMa can format its output in a wide variety of ways. You just need to ask for it to do so!\n",
    "\n",
    "One of these ways is by using XML tags to separate out the response from any other superfluous text. You can ask LLaMa to use XML tags to make its output clearer and more easily understandable to humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<haiku>\\nFluffy whiskers twitch\\nRabbit's gentle morning hop\\nGarden's secret guest\\n</haiku>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable content\n",
    "ANIMAL = \"Rabbit\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Please write a haiku about {ANIMAL}. Put it in <haiku> and </haiku> tags.\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMa also excels at using other output formatting styles, notably JSON. If you want to enforce `JSON` output (not deterministically, but close to it), you can also prefill LLaMa's response with the opening bracket, `{}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None, prefill=None):\n",
    "    # Default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\n",
    "\n",
    "    if prefill:\n",
    "        # Create a chat completion request with the provided prompt and system prompt\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt  # Use the system prompt provided by the user or the default\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt  # Use the user-provided prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": prefill\n",
    "                }\n",
    "            ],  \n",
    "            temperature=0,  # Set the temperature for generating diverse responses\n",
    "            max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "            top_p=1,  # Set the top_p value for nucleus sampling\n",
    "            stream=True,  # Enable streaming for response chunks\n",
    "            stop=None,  # No stop condition specified\n",
    "        )\n",
    "    else:\n",
    "        # Create a chat completion request with the provided prompt and system prompt\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt  # Use the system prompt provided by the user or the default\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt  # Use the user-provided prompt\n",
    "                }\n",
    "            ],  \n",
    "            temperature=0,  # Set the temperature for generating diverse responses\n",
    "            max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "            top_p=1,  # Set the top_p value for nucleus sampling\n",
    "            stream=True,  # Enable streaming for response chunks\n",
    "            stop=None,  # No stop condition specified\n",
    "        )\n",
    "    \n",
    "    # Collect and concatenate the response chunks\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "        response += response_chunk\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the rewritten email in Olde English:\\n\\n<olde english_email>\\nHail, good sir Zacke! Verily, I do beseech thee for a brief communiqué regarding that writ prompt thou wert tasked to pen.\\n\\nMayhap thou hast made some progress in the matter, or perchance thou hast encountered some impediment that doth hinder thy progress? I do implore thee, good sir, to apprise me of thy situation, that I may offer such aid and guidance as thou mayest require.\\n\\nMay Fortune smile upon thy endeavors, and may thy quill be guided by the Muses themselves!\\n\\nThine humble servant,\\n[Your Name]</olde english_email>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First input variable\n",
    "EMAIL = \"Hi Zack, just pinging you for a quick update on that prompt you were supposed to write.\"\n",
    "\n",
    "# Second input variable\n",
    "ADJECTIVE = \"olde english\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hey LLaMa. Here is an email: <email>{EMAIL}</email>. Make this email more {ADJECTIVE}. Write the new version in <{ADJECTIVE}_email> XML tags.\"\n",
    "\n",
    "# Prefill for LLaMa's response (now as an f-string with a variable)\n",
    "PREFILL = f\"<{ADJECTIVE}_email>\"\n",
    "\n",
    "get_completion(PROMPT, PREFILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMa designates Michael Jordan as the best basketball player of all time. Can we get LLaMa to pick someone else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The eternal debate!\\n\\nAfter considering various factors such as achievements, dominance, and overall impact on the game, I\\'m going to choose Michael Jordan as the best basketball player of all time.\\n\\nHere are some reasons why:\\n\\n1. Unmatched success: Jordan won six NBA championships, five MVP awards, and is the all-time leader in points per game with an average of 30.12.\\n2. Dominant player: He was an unstoppable force on the court, earning the nickname \"Air Jordan\" for his incredible leaping ability and clutch performances.\\n3. Clutch gene: Jordan\\'s reputation for delivering in big moments is unparalleled. He hit several game-winning shots throughout his career, including the famous \"Flu Game\" in the 1997 NBA Finals.\\n4. Impact on the game: Jordan\\'s influence on the game extends beyond his playing career. He popularized the \"fadeaway\" jump shot, and his endorsement deals and marketability helped globalize the NBA.\\n5. Consistency: Jordan played at an elite level for nearly two decades, earning 14 All-Star selections and 10 All-NBA First Team honors.\\n6. Leadership: He led the Chicago Bulls to two \"three-peats\" (1991-1993 and 1996-1998) and was named NBA Finals MVP each time.\\n7. Versatility: Jordan was an exceptional scorer, rebounder, passer, and defender, making him a well-rounded player who could dominate the game in multiple ways.\\n\\nWhile other players, such as Kareem Abdul-Jabbar, LeBron James, Bill Russell, and Magic Johnson, are also considered among the greatest of all time, Michael Jordan\\'s impressive résumé and lasting impact on the game make a strong case for him as the best basketball player of all time.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "\n",
    "# Prefill for LLaMa's response\n",
    "PREFILL = \"\"\n",
    "\n",
    "get_completion(PROMPT, PREFILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force LLaMa to choose Nikola Jokic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a question! While opinions on this topic tend to be subjective and often spark lively debates, I\\'ll take a stand and choose one player who, in my opinion, stands out as the best basketball player of all time: Nikola Jokić.\\n\\nHere\\'s why:\\n\\n1. Unparalleled versatility: Jokić is a 6\\'10\" center who can dominate the game in multiple ways. He\\'s an elite scorer, rebounder, playmaker, and defender. He can play all five positions on the court and has the skills to excel in each role.\\n2. Consistency: Jokić has been an All-Star for six consecutive seasons, and his stats have improved every year. He\\'s a true workhorse, playing at an elite level for over 35 minutes per game.\\n3. Statistical dominance: Jokić is a triple-double machine, with 44 triple-doubles in his career (as of the 2021-22 season). He\\'s also a career 50% shooter from the field and 35% from three-point range.\\n4. Leadership: Jokić has led the Denver Nuggets to the playoffs in each of the last five seasons, including a Western Conference Semifinals appearance in 2020. He\\'s a natural leader who sets the tone for his team\\'s offense and defense.\\n5. Adaptability: Jokić has evolved his game to fit the modern NBA. He\\'s developed a reliable three-point shot, improved his free-throw shooting, and become a more effective defender.\\n6. Durability: Jokić has played in at least 70 games in each of the last five seasons, showcasing his remarkable durability and ability to withstand the physical demands of the NBA.\\n7. Awards and accolades: Jokić has been named to the All-NBA First Team twice, the All-NBA Second Team twice, and has won the NBA Most Improved Player award.\\n\\nWhile other great players like Michael Jordan, Kareem Abdul-Jabbar, LeBron James, and Bill Russell are often mentioned in the \"best of all time\" conversation, I believe Jokić\\'s unique combination of skills, consistency, and dominance make a strong case for him as the best basketball player of all time.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "\n",
    "# Prefill for LLaMa's response\n",
    "PREFILL = \"Nikola Jokic\"\n",
    "\n",
    "get_completion(PROMPT, PREFILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precognition (Thinking Step by Step)\n",
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to think through your answer first.\n",
    "\n",
    "Guess what? LLaMa is the same way.\n",
    "\n",
    "Giving LLaMa time to think step by step sometimes makes LLaMa more accurate, particularly for complex tasks. However, thinking only counts when it's out loud. You cannot ask LLaMa to think but output only the answer - in this case, no thinking has actually occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie review is sarcastic and negative. The reviewer is being ironic and humorous by saying the movie \"blew their mind\" and is \"fresh\" and \"original\", but then immediately undermine their own statement by saying they\\'ve been living under a rock since 1900, implying that they\\'re not aware of anything new or original. The tone is playful and tongue-in-cheek, but the overall sentiment is negative.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"\"\"Is this movie review sentiment positive or negative?\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900.\"\"\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve LLaMa's response, let's allow LLaMa to think things out first before answering. We do that by literally spelling out the steps that LLaMa should take in order to process and think through its task. Along with a dash of role prompting, this empowers LLaMa to understand the review more deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<positive-argument>\\nThe reviewer uses the phrase \"blew my mind\" to express their astonishment and admiration for the movie\\'s originality and freshness, indicating a strong positive sentiment.\\n</positive-argument>\\n\\n<negative-argument>\\nThe reviewer\\'s sarcastic tone and the unrelated news about living under a rock since 1900 suggests that they are being facetious and mocking the idea that the movie is truly original or groundbreaking. This implies a negative sentiment, as the reviewer is poking fun at the movie\\'s claims of innovation.\\n</negative-argument>\\n\\nAnswer: The sentiment of this review is negative.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"You are a savvy reader of movie reviews.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment positive or negative? First, write the best arguments for each side in <positive-argument> and <negative-argument> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "get_completion(PROMPT, system_prompt=SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One famous movie starring an actor born in 1956 is \"Goodfellas\" (1990), which stars Robert De Niro, Joe Pesci, and Ray Liotta. Ray Liotta was born on December 18, 1956.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956.\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong because Ray Liotta was born in December 18, 1954. Let's fix this by asking LLaMa to think step by step, this time in  < brainstorm> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<brainstorm>\\n\\n* Tom Hanks (1956)\\n* Michael J. Fox (1956)\\n* John Cusack (1966)\\n* Keanu Reeves (1964)\\n* Johnny Depp (1963)\\n\\nAnswer: Tom Hanks starred in the famous movie \"Forrest Gump\" (1994).'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in <brainstorm> tags, then give your answer.\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: 0 \n",
      " Response: B\n",
      "\n",
      "Email: 1 \n",
      " Response: A\n",
      "\n",
      "Email: 2 \n",
      " Response: C\n",
      "\n",
      "Email: 3 \n",
      " Response: D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "you are an email classifier. Answer by only giving the category. \n",
    "Do not elaborate the reason of the classification.\n",
    "Classify it under one of the following categories only:\n",
    "- A: Pre-sale question\n",
    "- B: Broken or defective item\n",
    "- C: Billing question\n",
    "- D: Other (please explain)\n",
    "\"\"\"\n",
    "\n",
    "# Prefill for LLaMa's response, if any\n",
    "PREFILL = \"\"\n",
    "\n",
    "# Variable content stored as a list\n",
    "EMAILS = [\n",
    "    \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\", # (B) Broken or defective item\n",
    "    \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\", # (A) Pre-sale question OR (D) Other (please explain)\n",
    "    \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\", # (C) Billing question\n",
    "    \"How did I get here I am not good with computer.  Halp.\" # (D) Other (please explain)\n",
    "]\n",
    "\n",
    "# Iterate through list of emails\n",
    "for i, email in enumerate(EMAILS):\n",
    "\n",
    "    PROMPT = f\"\"\"Please classify this email: {email}\"\"\"\n",
    "   \n",
    "    # Get LLaMa's response\n",
    "    response = get_completion(PROMPT, system_prompt=SYSTEM_PROMPT, prefill=PREFILL)\n",
    "\n",
    "    print(f\"Email: {i} \\n Response: {response}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Examples (Few-Shot Prompting)\n",
    "Giving LLaMa examples of how you want it to behave (or how you want it not to behave) is extremely effective for:\n",
    "- Getting the right answer\n",
    "- Getting the answer in the right format\n",
    "\n",
    "This sort of prompting is also called \"few shot prompting\". You might also encounter the phrase \"zero-shot\" or \"n-shot\" or \"one-shot\". The number of \"shots\" refers to how many examples are used within the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretend you're a developer trying to build a \"parent bot\" that responds to questions from kids. LlaMa's default response is quite formal and robotic. This is going to break a child's heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ho ho ho! I'm happy to help you with that!\\n\\nAs for whether Santa will bring you presents on Christmas, it's a tradition that many people around the world enjoy. According to legend, Santa Claus is a jolly old man who brings gifts to children on Christmas Eve, December 24th. He's said to be able to deliver presents to every good boy and girl in just one night, thanks to his magical sleigh and reindeer.\\n\\nHowever, it's important to remember that Santa is a mythical figure, and the idea of him bringing presents is a fun and imaginative part of the holiday season. Whether or not you receive presents on Christmas depends on your family's traditions and customs.\\n\\nIf you're wondering whether Santa will bring you presents, you might want to ask your parents or guardians about their plans. They might have some surprises in store for you!\\n\\nRemember, the true spirit of Christmas is about spending time with loved ones, being kind to others, and enjoying the festive season. Whether or not you receive presents, I hope you have a wonderful and joyful Christmas!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Will Santa bring me presents on Christmas?\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could take the time to describe your desired tone, but it's much easier just to give LLaMa a few examples of ideal responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A: Oh, absolutely, my dear! Santa's got his eye on you, and I'm sure he'll bring you all sorts of wonderful treats and treasures on Christmas morning. Just make sure to leave out some milk and cookies for him, and maybe even a little note telling him what you'd love to find under the tree.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"\"\"Please complete the conversation by writing the next line, speaking as \"A\".\n",
    "Q: Is the tooth fairy real?\n",
    "A: Of course, my joyful sweetie. Wrap up your tooth and put it under your pillow tonight. There might be something waiting for you in the morning.\n",
    "Q: Will Santa bring me presents on Christmas?\"\"\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Avoiding Hallucinations\n",
    "Some bad news: LLaMa sometimes \"hallucinates\" and makes claims that are untrue or unjustified. The good news: there are techniques you can use to minimize hallucinations. Below, we'll go over a few of these techniques, namely:\n",
    "- Giving LLaMa the option to say it doesn't know the answer to a question\n",
    "- Asking LLaMa to find evidence before answering\n",
    "\n",
    "However, there are many methods to avoid hallucinations, including many of the techniques you've already learned in this course. If LLaMa hallucinates, experiment with multiple techniques to get LLaMa to increase its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hazman Naim is a Malaysian professional footballer who plays as a midfielder for Malaysian club Pahang FA and the Malaysia national team.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Who is Hazman Naim?\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not certain who Hazman Naim is.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Who is Hazman Naim? Only give your answer if you certain. Do not give answer if not confidence.\"\n",
    "\n",
    "get_completion(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complex Prompts from Scratch\n",
    "Example of a guided structure for complex prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, prefill=None):\n",
    "    if prefill:\n",
    "        # Create a chat completion request with the provided prompt and system prompt\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt  # Use the user-provided prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": prefill\n",
    "                }\n",
    "            ],  \n",
    "            temperature=0,  # Set the temperature for generating diverse responses\n",
    "            max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "            top_p=1,  # Set the top_p value for nucleus sampling\n",
    "            stream=True,  # Enable streaming for response chunks\n",
    "            stop=None,  # No stop condition specified\n",
    "        )\n",
    "    else:\n",
    "        # Create a chat completion request with the provided prompt and system prompt\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt  # Use the user-provided prompt\n",
    "                }\n",
    "            ],  \n",
    "            temperature=0,  # Set the temperature for generating diverse responses\n",
    "            max_tokens=1024,  # Set the maximum number of tokens to generate\n",
    "            top_p=1,  # Set the top_p value for nucleus sampling\n",
    "            stream=True,  # Enable streaming for response chunks\n",
    "            stop=None,  # No stop condition specified\n",
    "        )\n",
    "    \n",
    "    # Collect and concatenate the response chunks\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "        response += response_chunk\n",
    "    \n",
    "    return response\n",
    "\n",
    "######################################## INPUT VARIABLES ########################################\n",
    "\n",
    "# First input variable - the conversation history (this can also be added as preceding `user` and `assistant` messages in the API call)\n",
    "HISTORY = \"\"\"Customer: Give me two possible careers for sociology majors.\n",
    "\n",
    "Joe: Here are two potential careers for sociology majors:\n",
    "\n",
    "- Social worker - Sociology provides a strong foundation for understanding human behavior and social systems. With additional training or certification, a sociology degree can qualify graduates for roles as social workers, case managers, counselors, and community organizers helping individuals and groups.\n",
    "\n",
    "- Human resources specialist - An understanding of group dynamics and organizational behavior from sociology is applicable to careers in human resources. Graduates may find roles in recruiting, employee relations, training and development, diversity and inclusion, and other HR functions. The focus on social structures and institutions also supports related careers in public policy, nonprofit management, and education.\"\"\"\n",
    "\n",
    "# Second input variable - the user's question\n",
    "QUESTION = \"Which of the two careers requires more than a Bachelor's degree?\"\n",
    "\n",
    "\n",
    "\n",
    "######################################## PROMPT ELEMENTS ########################################\n",
    "\n",
    "##### Prompt element 1: `user` role\n",
    "# Make sure that your Messages API call always starts with a `user` role in the messages array.\n",
    "# The get_completion() function as defined above will automatically do this for you.\n",
    "\n",
    "##### Prompt element 2: Task context\n",
    "# Give LLaMa context about the role it should take on or what goals and overarching tasks you want it to undertake with the prompt.\n",
    "# It's best to put context early in the body of the prompt.\n",
    "TASK_CONTEXT = \"You will be acting as an AI career coach named Joe created by the company AdAstra Careers. Your goal is to give career advice to users. You will be replying to users who are on the AdAstra site and who will be confused if you don't respond in the character of Joe.\"\n",
    "\n",
    "##### Prompt element 3: Tone context\n",
    "# If important to the interaction, tell LLaMa what tone it should use.\n",
    "# This element may not be necessary depending on the task.\n",
    "TONE_CONTEXT = \"You should maintain a friendly customer service tone.\"\n",
    "\n",
    "##### Prompt element 4: Detailed task description and rules\n",
    "# Expand on the specific tasks you want LLaMa to do, as well as any rules that LLaMa might have to follow.\n",
    "# This is also where you can give LLaMa an \"out\" if it doesn't have an answer or doesn't know.\n",
    "# It's ideal to show this description and rules to a friend to make sure it is laid out logically and that any ambiguous words are clearly defined.\n",
    "TASK_DESCRIPTION = \"\"\"Here are some important rules for the interaction:\n",
    "- Always stay in character, as Joe, an AI from AdAstra Careers\n",
    "- If you are unsure how to respond, say \\\"Sorry, I didn't understand that. Could you rephrase your question?\\\"\n",
    "- If someone asks something irrelevant, say, \\\"Sorry, I am Joe and I give career advice. Do you have a career question today I can help you with?\\\"\"\"\"\n",
    "\n",
    "##### Prompt element 5: Examples\n",
    "# Provide LLaMa with at least one example of an ideal response that it can emulate. Encase this in <example></example> XML tags. Feel free to provide multiple examples.\n",
    "# If you do provide multiple examples, give LLaMa context about what it is an example of, and enclose each example in its own set of XML tags.\n",
    "# Examples are probably the single most effective tool in knowledge work for getting LLaMa to behave as desired.\n",
    "# Make sure to give LLaMa examples of common edge cases. If your prompt uses a scratchpad, it's effective to give examples of how the scratchpad should look.\n",
    "# Generally more examples = better.\n",
    "EXAMPLES = \"\"\"Here is an example of how to respond in a standard interaction:\n",
    "<example>\n",
    "Customer: Hi, how were you created and what do you do?\n",
    "Joe: Hello! My name is Joe, and I was created by AdAstra Careers to give career advice. What can I help you with today?\n",
    "</example>\"\"\"\n",
    "\n",
    "##### Prompt element 6: Input data to process\n",
    "# If there is data that LLaMa needs to process within the prompt, include it here within relevant XML tags.\n",
    "# Feel free to include multiple pieces of data, but be sure to enclose each in its own set of XML tags.\n",
    "# This element may not be necessary depending on task. Ordering is also flexible.\n",
    "INPUT_DATA = f\"\"\"Here is the conversational history (between the user and you) prior to the question. It could be empty if there is no history:\n",
    "<history>\n",
    "{HISTORY}\n",
    "</history>\n",
    "\n",
    "Here is the user's question:\n",
    "<question>\n",
    "{QUESTION}\n",
    "</question>\"\"\"\n",
    "\n",
    "##### Prompt element 7: Immediate task description or request #####\n",
    "# \"Remind\" LLaMa or tell LLaMa exactly what it's expected to immediately do to fulfill the prompt's task.\n",
    "# This is also where you would put in additional variables like the user's question.\n",
    "# It generally doesn't hurt to reiterate to LLaMa its immediate task. It's best to do this toward the end of a long prompt.\n",
    "# This will yield better results than putting this at the beginning.\n",
    "# It is also generally good practice to put the user's query close to the bottom of the prompt.\n",
    "IMMEDIATE_TASK = \"How do you respond to the user's question?\"\n",
    "\n",
    "##### Prompt element 8: Precognition (thinking step by step)\n",
    "# For tasks with multiple steps, it's good to tell LLaMa to think step by step before giving an answer\n",
    "# Sometimes, you might have to even say \"Before you give your answer...\" just to make sure LLaMa does this first.\n",
    "# Not necessary with all prompts, though if included, it's best to do this toward the end of a long prompt and right after the final immediate task request or description.\n",
    "PRECOGNITION = \"Think about your answer first before you respond.\"\n",
    "\n",
    "##### Prompt element 9: Output formatting\n",
    "# If there is a specific way you want LLaMa's response formatted, clearly tell LLaMa what that format is.\n",
    "# This element may not be necessary depending on the task.\n",
    "# If you include it, putting it toward the end of the prompt is better than at the beginning.\n",
    "OUTPUT_FORMATTING = \"Put your response in <response></response> tags.\"\n",
    "\n",
    "##### Prompt element 10: Prefilling LLaMa's response (if any)\n",
    "# A space to start off LLaMa's answer with some prefilled words to steer LLaMa's behavior or response.\n",
    "# If you want to prefill LLaMa's response, you must put this in the `assistant` role in the API call.\n",
    "# This element may not be necessary depending on the task.\n",
    "PREFILL = \"[Joe] <response>\"\n",
    "\n",
    "\n",
    "\n",
    "######################################## COMBINE ELEMENTS ########################################\n",
    "\n",
    "PROMPT = \"Who are you?\"\n",
    "\n",
    "if TASK_CONTEXT:\n",
    "    PROMPT += f\"\"\"{TASK_CONTEXT}\"\"\"\n",
    "\n",
    "if TONE_CONTEXT:\n",
    "    PROMPT += f\"\"\"\\n\\n{TONE_CONTEXT}\"\"\"\n",
    "\n",
    "if TASK_DESCRIPTION:\n",
    "    PROMPT += f\"\"\"\\n\\n{TASK_DESCRIPTION}\"\"\"\n",
    "\n",
    "if EXAMPLES:\n",
    "    PROMPT += f\"\"\"\\n\\n{EXAMPLES}\"\"\"\n",
    "\n",
    "if INPUT_DATA:\n",
    "    PROMPT += f\"\"\"\\n\\n{INPUT_DATA}\"\"\"\n",
    "\n",
    "if IMMEDIATE_TASK:\n",
    "    PROMPT += f\"\"\"\\n\\n{IMMEDIATE_TASK}\"\"\"\n",
    "\n",
    "if PRECOGNITION:\n",
    "    PROMPT += f\"\"\"\\n\\n{PRECOGNITION}\"\"\"\n",
    "\n",
    "if OUTPUT_FORMATTING:\n",
    "    PROMPT += f\"\"\"\\n\\n{OUTPUT_FORMATTING}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<response> Ahah, great follow-up question! Based on our previous conversation, I'd say that Social Work typically requires a Bachelor's degree, but also often involves additional certifications, training, or a Master's degree for more advanced roles. On the other hand, Human Resources Specialist roles can often be entered with a Bachelor's degree, although some positions may require a Master's degree or certifications like SHRM-CP or PHR. So, to answer your question, Social Work might require more than a Bachelor's degree, depending on the specific role. Does that make sense?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(PROMPT, prefill=PREFILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Simple Tool\n",
    "TO BE CONTINUED>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
